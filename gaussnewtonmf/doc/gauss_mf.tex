\documentclass[11pt,twoside]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx, amssymb}
\usepackage{caption}
\usepackage{subfig}
%\usepackage{subcaption}
\usepackage{amsmath,mathtools}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{physics}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow,array}

\usepackage{xfrac}
\usepackage{xcolor}

\usepackage{verbatim}
\usepackage{natbib}

\newcommand{\bsym}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\bw}{\ensuremath{\bsym{w}}}
\newcommand{\bd}{\ensuremath{\bsym{d}}}
\newcommand{\bx}{\ensuremath{\bsym{x}}}
\newcommand{\bj}{\ensuremath{\bsym{j}}}
\newcommand{\bp}{\ensuremath{\bsym{p}}}
\newcommand{\bq}{\ensuremath{\bsym{q}}}
\newcommand{\bz}{\ensuremath{\bsym{z}}}
\newcommand{\bb}{\ensuremath{\bsym{b}}}
\newcommand{\by}{\ensuremath{\bsym{y}}}
\newcommand{\byy}{\ensuremath{\bsym{\tilde{y}}}}

\newcommand{\bh}{\ensuremath{\bsym{h}}}


\newcommand{\bu}{\ensuremath{\bsym{u}}}
\newcommand{\bv}{\ensuremath{\bsym{v}}}


\newcommand{\bs}{\ensuremath{\bsym{s}}}
\newcommand{\bg}{\ensuremath{\bsym{g}}}


\newcommand{\bbr}{\ensuremath{\mathcal R}}
\newcommand{\half}{\ensuremath{\frac{1}{2}}}
\newcommand{\halfhalf}{\ensuremath{\frac{1}{4}}}
\newcommand{\pointprod}[2]{\ensuremath{\left( #1 \operatorname{\prescript{}{\cdot}{\ast}} #2 \right)}}
\newcommand{\bbO}[1]{\ensuremath{\mathcal{O}\left(#1\right)}}

\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\vectorize}{vec}


\begin{document}
\begin{center}
    {\Large Newton Methods for Factorization Machines}
\end{center}


\section{Introduction}
Given a rating matrix R,  matrix factorization (MF) is a technique to find two dense matrices $U \in \bbr^{d \times M}$ and $V \in \bbr^{d \times N}$ such that $r_{m,n} \simeq \bu_m^T\bv_n$, where $\bu_m \in \bbr^d$ and $\bv_n \in \bbr^d$ are respectively the $m$th column of $U$ and the $n$th column of $V$, $d$ is the pre-speciﬁed number of latent features, and the entry $r_{m,n}$ denotes the feedback of the $m$th user on $n$th item.  This task is achieved by solving the following non-convex problem
%To determines MF's parameters, we solve the following optimization problem.
\begin{equation}
    %\min_{U, V} \quad f(U, V)
    \min_{U,V} \quad \frac{1}{2}\sum_{(m, n) \in R}  (r_{m,n} - \bu_m^T \bv_n)^2+
    \frac{\lambda}{2} (\|U\|^2_F + \|V\|^2_F	)
    \label{eq:MF}
\end{equation}
where $(m, n) \in R$ indicates that rating $r_{m,n}$ is available, and $\lambda$ is regularization coeﬃcients for avoiding over-ﬁtting.
%$R$ is is the rating matrix that includes the feedback of the $m$th user on $n$th item at the ($m$, $n$) entry $r_{m n}$. %The matrix factorization is technique to find two dense matrices $U \in \bbr^{d \times M}$ and $V \in \bbr^{d \times N}$ such that $r_{m,n} = \bu_m^T\bv_n$, where $\bu_m \in \bbr^d$ and $\bv_n \in \bbr^d$ are respectively the $m$th column of $U$ and the $n$th column of $V$.  

In fact, the problem \eqref{eq:MF} is a special case of Factorization Machines (FM). In order to show the connection between MF and FM and reuse the former derivation, we apply formulations for \eqref{eq:MF} and have

\begin{equation}
\min_{U, V} \quad f(U, V)\label{eq:reMF}
\end{equation}
where 
\begin{equation}
    f(U, V) = \frac{1}{2}\sum_{i}^l  \Bigl(y_i - (U \bw_i)^T (V \bh_i)\Bigr)^2+
    \frac{\lambda}{2} (\|U\|^2_F +\|V\|^2_F	)
    \label{eq:min_reMF}
\end{equation}
and $l$ is the number of nonzero entries in $R$. We can consider $(y_i, \bw_i, \bh_i)$ as $i$th instance of a regression problem which totally has $l$ instances in the training set.
$y_i=r_{m, n}$ is the value of the corresponding rating from $R$. $\bh_i \in \bbr^N $ and $\bw_i \in \bbr^M$ are the feature vectors. For MF problem, $\bw_i$ and $\bh_i$  include a single user id and a single item id and are encoded by one-hot, respectively.  
Note that \eqref{eq:reMF} is a variant FM that \citet{MB16a} proposed. 
In comparison with standard FM \citep{SR10c}, 
it is allowed to encode the feature representations in two latent spaces by $U$ and $V$ respectively in our problem. 
\citet{MB16a} mentioned that the function of \eqref{eq:reMF} is a multi-block convex function of $U$ and $V$ so the idea of alternating minimization by using any convex optimization algorithm to solve each sub-problem can be applied. In this work, we focus on solving MF problem. Because it can be regarded as a special case of the variant FM, we mainly study the variant FM in the following.


To solve \eqref{eq:reMF}, \citet{test17a} proposed an alternating Newton method (ANT) which is a variant alternating least squares (ALS) and more efficient than stochastic gradient methods and coordinate descent methods. ANT alternately switches between solving these two sub-problems, namely minimizing over $U$ while keeping $V$ fixed and minimizing over $V$ while keeping $U$ fixed. 
%Note that because \eqref{eq:MF} is a special case of \eqref{eq:reMF}, ANT also can solve \eqref{eq:MF}.
%However, we observe an imbalance problem that one of the sub-problems needs a further number of iterations of conjugate gradient method (CG) than the other sub-problem. That is, it is harder to solve one of the sub-problems than the other sub-problem. We guess that solving these two sub-problems at the same time may get better performance, so we try to apply Gauss-Newton method to solve the problem. 
%In addition, we also try to apply ALS to solve it to observe the extreme case of the imbalance problem.

However, \citet{test17a} showed that test performance of the alternating minimization dose not meet expectation on imbalance data, such as {\tt avazu-app}, {\tt avazu-site} and {\tt criteo}. We guess that the method, alternately solving two sub-problems, is not suitable for solving imbalance data because one of sub-problem may dominate whole problem. Therefore, we try to solve these two sub-problems at the same time by applying Gauss-Newton (GN) method. In other words, GN can update $U$ and $V$ in \eqref{eq:reMF} per iteration instead of updating only one of them per iteration. In this work, 
%we compare performance among ALS, ANT and GN for solving \eqref{eq:reMF} on imbalance data. 
we aim at investigating how to apply GN to solve MF problem and showing that GN can achieve better performance than ANT and ALS for solving \eqref{eq:reMF} on imbalance data.

\section{Related works}
\section{Gauss-Newton method for learning Factorization Machines}
For the standard Newton methods, at the $k$th iteration, we find a direction $\bs_k$ minimizing the following second-order approximation of the function value:
\begin{equation}
    \min_{\bs} \quad \frac{1}{2}\bs^T H_k \bs + \bg_k^T \bs
\label{eq:QP}
\end{equation}
where $H_k = \nabla^2 f(U_k, V_k)$ and $\bg_k = \nabla f(U_k, V_k)$ are the Hessian matrix and the gradient of $f(U_k, V_k)$, respectively.
If $H_k$ is positive definite, then \eqref{eq:QP} is equivalent to solving the following linear system:
\begin{equation}
    H_k\bs = -\bg_k
\label{eq:HLE}
\end{equation}

Unfortunately, the convexity of \eqref{eq:reMF} brings some difficulties in developing a Newton-type FM solver.
The Hessian matrix of \eqref{eq:reMF}  is not alway positive-definite, so a direct application of Newton method may be failed to find a descent direction.
We may consider a positive-definite approximation of the Hessian matrix such as Gauss-Newton method. 
That is, we remove the last term in \eqref{eq:H} and obtain the following positive-definite matrix.
\begin{equation}
    \begin{aligned}
    G &= \lambda I + \sum_{i=1}^{l}\bj_i\bj_i^T
    \label{eq:G}
    \end{aligned}
\end{equation}

The Gauss-Newton method can be viewed as a modification of Newton method with line search. Instead of solving \eqref{eq:HLE}, we solve the following linear system to find a $\bs_k$ for factorization machines problem
\begin{equation}
	G_k\bs = -\bg_k
\label{eq:GLE}
\end{equation}
where $G_k$ is the Gauss-Newton matrix at $k$th iteration.


In practice, \eqref{eq:GLE} may not need to exactly solved. Therefore, truncated Newton methods have been introduced to approximately solve \eqref{eq:GLE}. Iterative methods such as conjugate gradient method are often used for approximately solving \eqref{eq:GLE}, so at each iteration an inner iterative process is involved.

Another difficulty to solve \eqref{eq:GLE} is that storing the Gauss-Newton matrix is not possible if the dimensionality is very large. To overcome the memory issue. 
%Some previous works showed that for linear problems, the conjugate gradient method that mainly requires a sequence of Hessian-vector products can performed without explicitly forming the Hessian matrix:
Some previous works showed that for matrices in a form similar to \eqref{eq:G}, the matrix-vector product at each step in conjugate gradient (CG) method can be conducted without explicitly forming the Hessian matrix:
\begin{equation}
    H\bs = \lambda\bs + X^T(D(X^T\bs))
\label{eq:Hv}
\end{equation}
where $X$ is the data matrix and $\bs$ is the direction in the CG procedure. 

After a direction is found in our Newton method, we must decide the step size taken along that direction. To guarantee a sufficient objective function reduction, here we consider a back-tracking line search to find the largest step size $\theta\in\{1,\beta,\beta^2,\dots\}$ satisfying Armijo rule,
\begin{equation}
    f(U+\theta S_{u},V+\theta S_{v}) - f(U,V) \le \theta\nu gs,
    \label{eq:LineSearchRule}
\end{equation}
where $\nu\in(0,1)$ and $\beta\in(0,1)$ are pre-specified constants.
Recalculating the function value at each $U+\theta S_{u}$ and $V+\theta S_{v}$ is expensive, where the main cost is on calculating $((U+\theta S_{u})\bw_i)^T((V+\theta S_{v})\bh_i)$, $\forall i$.
However, the following trick in \cite{GXY09a} can be employed.
Assume that
\begin{equation}
    \tilde{y}_i = (U\bw_i)^T(V\bh_i), \tilde{\Delta}_i=(S_u\bw_i)^T(V\bh_i)+(U\bw_i)^T(S_v\bh_i)\text{ and }\tilde{\Delta}_i^\prime = (S_u\bw_i)^T(S_v\bh_i),\; i=1,\dots,l    
    \label{eq:LsRequire}
\end{equation}
are available.
At an arbitrary $\theta$, we can calculate
\begin{equation}
    \left((U+\theta S_u)\bw_i\right)^T\left((V+\theta S_v)\bh_i\right) = \tilde{y}_i + \theta\tilde{\Delta}_i + {\theta}^2\tilde{\Delta}_i^\prime    
    \label{eq:ExpTheta}
\end{equation}
to get the new output value.
By further maintaining
\begin{equation}    
    \langle{U}{,}{S_u}\rangle,\langle{V}{,}{S_v}\rangle,\|S\|_F^2,\text{ and } {\bg}^T{\bs},
    \label{eq:LsRequire1}
\end{equation}
the total cost of checking the condition of \eqref{eq:LineSearchRule} is merely $\bbO{l}$ because
\begin{equation}
    \begin{aligned}
        f(U+\theta S_{u},V+\theta S_{v}) - f(U,V) =& \frac{\lambda}{2}\left( 2\theta \langle{U}{,}{S_u}\rangle + 2\theta \langle{V}{,}{S_v}\rangle + \theta^2\|S\|_F^2 \right)\\ 
        &+ \sum_{i=1}^l \xi(\tilde{y}_i + \theta\tilde{\Delta}_i + {\theta}^2\tilde{\Delta}_i^\prime;y_i) -\sum_{i=1}^l \xi(\tilde{y}_i;y_i)        
    \end{aligned}
    \label{eq:LrObjFast}
\end{equation}
where $\langle\cdot{,}\cdot\rangle$ is the inner product of two matrices.

To compute the gradient of \eqref{eq:reMF}, we begin with the following properties of  the Kronecker product among matrices $A$, $B$ and $X$:
\begin{equation}
(B^T \otimes A)\vectorize(X) = \vectorize(AXB)
\label{eq:kronecker_vec}
\end{equation}
\begin{equation}
(A \otimes B)^T = A^T \otimes B^T
\label{eq:kronecker_T}.
\end{equation}
Moreover, the predicted value can be represented as
\begin{align}
(U \bw_i)^T (V \bh_i) &= (\bh_i^T \otimes (U \bw_i)^T)\vectorize(V)\label{eq:predict_UwVh_T} \\
&= (\bh_i \otimes (U \bw_i))^T\vectorize(V)
\label{eq:predict_UwVh}
\end{align}
where \eqref{eq:predict_UwVh_T} and \eqref{eq:predict_UwVh} are from \eqref{eq:kronecker_vec} and \eqref{eq:kronecker_T}, respectively.
The predicted value also can be represented as
\begin{align}
(U \bw_i)^T (V \bh_i) &= (V \bh_i)^T (U \bw_i) \nonumber \\
&= (\bw_i^T \otimes (V \bh_i)^T)\vectorize(U)\nonumber \\
&= (\bw_i \otimes (V \bh_i))^T \vectorize(U)
\label{eq:predict_VhUw}.
\end{align}
Then, we define $\bp_i = U\bw_i$, $\bq_i = V\bh_i$, $z_i = \bp_i^T\bq_i$, $b_i = z_i-y_i = \bp_i^T\bq_i-y_i$, the loss function $\xi_i = b_i^2 = \Bigl(\bp_i^T\bq_i-y_i\Bigr)^2$, $Q=[\bq_1,\dots,\bq_l] \in \bbr^{d\times l}$, $W=[\bw_1,\dots,\bw_l]^T \in \bbr^{l\times M}$, $P=[\bp_1,\dots,\bp_l] \in \bbr^{d\times l}$, and $H=[\bh_1,\dots,\bh_l]^T \in \bbr^{l\times N}$.
The gradient of \eqref{eq:reMF} is
\begin{align}
\nabla {f}(\vectorize(U, V)) = \lambda \vectorize(U,V) + \frac{1}{2}\sum_{i=1}^{l}\pdv{\xi_i}{z_i}\begin{bmatrix} \frac{\partial z_i}{\partial \vectorize(U)} \\ \frac{\partial z_i}{\partial \vectorize(V)} \end{bmatrix}
\label{eq:gradUV}.
\end{align}
We define the following Jocobian of $z_i$ is
\begin {align}
%\bj_i = (\bw_i\otimes \bq_i, \bh_i\otimes \bp_i)
\bj_i &= \begin{bmatrix} \frac{\partial z_i}{\partial \vectorize(U)} \\ \frac{\partial z_i}{\partial \vectorize(V)} \end{bmatrix}\nonumber \\
&= \begin{bmatrix} \frac{\partial (\bw_i \otimes (V \bh_i))^T\vectorize(U)}{\partial \vectorize(U)} \\ \frac{\partial (\bh_i \otimes (U \bw_i))^T\vectorize(V)}{\partial \vectorize(V)} \end{bmatrix}\nonumber \\
&= \begin{bmatrix} \bw_i\otimes \bq_i \\ \bh_i\otimes \bp_i \end{bmatrix}
%&= \begin{bmatrix} \bw_i\otimes \bq_i \\ \bh_i\otimes \bp_i \end{bmatrix}_{(Md+Nd)\times 1}
\label{eq:Jacob}.
\end{align}
From \eqref{eq:gradUV} and \eqref{eq:Jacob}, the gradient is
\begin {align}
\nabla {f}(\vectorize(U, V)) 
&= \lambda \vectorize(U,V) + \sum_{i=1}^l b_i \bj_i\nonumber\\
&= \lambda \vectorize(U,V) + \begin{bmatrix} \sum_{i=1}^l b_i (\bw_i\otimes\bq_i) \\ \sum_{i=1}^l b_i (\bh_i\otimes\bp_i)\end{bmatrix}\label{eq:GradU_K1}\\
&= \lambda \vectorize(U,V) + \begin{bmatrix} \sum_{i=1}^l \vectorize\left(\bq_i b_i \bw_i^T \right) \\ \sum_{i=1}^l \vectorize\left(\bp_i b_i \bh_i^T \right)\end{bmatrix}\label{eq:GradU_K2}\\
&= \lambda \vectorize(U,V) + \begin{bmatrix} \vectorize\left( \sum_{i=1}^l \bq_i b_i \bw_i^T \right) \\ \vectorize\left( \sum_{i=1}^l \bp_i b_i \bh_i^T \right)\end{bmatrix}\nonumber\\
&= \lambda \vectorize(U,V) + \begin{bmatrix} \vectorize \Bigl ( Q \bigl(\diag(\bsym{b}) W \bigr)  \Bigr) \\ \vectorize \Bigl ( P \bigl(\diag(\bsym{b}) H \bigr)  \Bigr)\end{bmatrix}\nonumber\\
\label{eq:Grad},
\end{align}
where $\diag(\bsym{b})$ is a diagonal matrix in which the $i$th diagonal element is $b_i$, and for the second term in \eqref{eq:GradU_K1} and \eqref{eq:GradU_K2} we apply \eqref{eq:kronecker_vec}. 
Since $W$ and $H$ is highly sparse matrices, we need to calculate $\diag(\bsym{b})W$ and $\diag(\bsym{b})H$ first.
Then, the main operation for gradient evaluation is a matrix-matrix product that costs $\bbO{2d\times l}$.
\par

The Hessian matrix of \eqref{eq:reMF} is 
\begin{align}
\pdv{\vectorize(U,V)^T} \frac{\partial f}{\partial \vectorize(U,V)}
&=\lambda I + \sum_{i=1}^{l}\pdv{\vectorize(U,V)^T} (b_i \bj_i)\nonumber\\
&=\lambda I + \sum_{i=1}^{l} \Bigl( \pdv{b_i^T} (b_i \bj_i) \frac{\partial b_i}{\partial \vectorize(U,V)^T} + \pdv{\bj_i^T} (b_i \bj_i) \frac{\partial \bj_i}{\partial \vectorize(U,V)^T} \Bigr)\nonumber\\
&=\lambda I + \sum_{i=1}^{l} \Bigl( \bj_i \frac{\partial b_i}{\partial z_i} \frac{\partial z_i}{\partial \vectorize(U,V)^T} + b_i I \frac{\partial \bj_i}{\partial \vectorize(U,V)^T} \Bigr)\nonumber\\
&=\lambda I + \sum_{i=1}^{l} \bj_i \bj_i^T + \sum_{i=1}^{l} b_i \frac{\partial \bj_i}{\partial \vectorize(U,V)^T} 
\label{eq:H}
\end{align}
where $I$ is the identity matrix.

Specifically, from (\ref{eq:G}) and $\bs=\vectorize(S)$, we first calculate
\begin{equation}
\begin{aligned}
    a_i&= \bj_i^T \vectorize(S)\\
    &=\begin{bmatrix} \bw_i^T\otimes\bq_i^T & \bh_i^T\otimes\bp_i^T \end{bmatrix}\begin{bmatrix}\vectorize(S_u)\\ \vectorize(S_v)\end{bmatrix}\\
    &=\bq_i^T S_u \bw_i + \bp_i^T S_v \bh_i, \text{ }i=1,\dots,l
    \label{eq:DefZi}
\end{aligned}
\end{equation}
or equivalently
\begin{equation}
    \textbf{\textit{a}} = \biggl({Q^T}\odot{(WS_u^T)}+{P^T}\odot{(HS_v^T)}\biggr) \bsym{1}_{d\times 1}
    \label{eq:CalcZ},
\end{equation}
where $\odot$ is the Hadamard product of two matrices.  Next,
\begin{align}
    G \bs         &= \lambda \vectorize(S) + \sum_{i=1}^l a_i \bj_i \\
                  &= \lambda \vectorize(S) + \sum_{i=1}^l a_i \begin{bmatrix} \bw_i\otimes \bq_i \\ \bh_i\otimes \bp_i \end{bmatrix} \nonumber \\
                  &= \lambda \vectorize(S) +  \sum_{i=1}^l \begin{bmatrix}\vectorize\left(a_i \bq_i \bw_i^T\right) \\ \vectorize\left(a_i \bp_i \bh_i^T\right)\end{bmatrix} \nonumber  \\
                  &= \lambda \vectorize(S) +  \begin{bmatrix}\vectorize \Bigl ( Q \bigl(\diag(\textbf{\textit{a}}) W \bigr) \Bigr)\\\vectorize \Bigl ( P \bigl ( \diag(\textbf{\textit{a}}) H \bigr) \Bigr)\end{bmatrix} \label{eq:Hv},
\end{align}

We investigate the complexity of the Hessian-vector product. The first term of 
\eqref{eq:Hv} is a standard vector scaling that cost \bbO{(M+N) \times d}.
For the second term in (\ref{eq:Hv}), we separately consider two major steps:
\begin{enumerate}
    \item The computation of $WS_u^T$ and $HS_v^T$. $\bbO{d \times l}$
    \item The element-wise product between $P^T$ and $HS_v^T$.  $\bbO{ d \times l}$
    \item The product between dense vector $Q$ and $\diag{(\bsym{z})}W$. $\bbO{d \times l}$
\end{enumerate}
Therefore, the cost of one Hessian-vector  is $\bbO{d \times l}$. After $\bs_k$ is obtained from conjugate gradient method, a backtracking line search is applied to adjust step size $\alpha$, the complexity of the line search is
\begin{equation*}
\bbO{2d \times l} + (\text{\# of line search iterations})\times \bbO{l}.
\end{equation*}

\clearpage\newpage
%\section{Algorithm}
\begin{algorithm}
    \caption{An implementation for solving \eqref{eq:reMF} by operations on matrix veriables without vectorizing them.}
    \label{alg:LrFramework}
    \begin{algorithmic}[1]
        \State Given $0< \epsilon < 1$ and the current $(U,V)$.
        \State $Q \gets VH^T$, $P \gets UW^T$
        \State Compute and cache $\bsym{\tilde{y}}= (Q \odot P)^T\bsym{1}_{d\times 1}$.
        \State $\bb \leftarrow \byy - \by $
        %\State $f \gets \frac{\lambda}{2}(\|U\|_F^2 + \|V\|_F^2) + \frac{1}{2}\sum_{i=1}^l {y_i - \tilde{y}_i}$.
        \State $f \gets \frac{\lambda}{2}(\|U\|_F^2 + \|V\|_F^2) + \frac{1}{2}\sum_{i=1}^l \xi(\tilde{y}_i;y_i)$.
        \For {$k \gets \{0,1,\dots\}$}
%            \State Calculate and store $\tilde{y}_i$ and then obtain $b_i$, $\forall i$.
            %\State Calculate and store $\tilde{y}_i$ and then obtain $b_i$ by \eqref{eq:LossD}, $\forall i$.
            \State $G \gets \lambda \begin{bmatrix} U \, V \end{bmatrix} + \begin{bmatrix} Q \bigl(\diag(\bsym{b}) W \bigr) \, P \bigl ( \diag(\bsym{b}) H \bigr) \end{bmatrix}$
            \If {$k=0$}
                \State $\|G^0\|_F \gets \|G\|_F$
            \EndIf
            \If {$\|G\|_F \le \epsilon\|G^0\|_F$}
                \State Output $\begin{bmatrix} U \, V \end{bmatrix}$ as the solution of \eqref{eq:reMF}.
                \State {\bf break}
            \EndIf
            %\State Compute $D_{ii}$ via \eqref{eq:LossDD}, $\forall i$.
            \State Solve (\ref{eq:HLE}) approximately via CG to get an update direction $S =  \begin{bmatrix} S_u \, S_v \end{bmatrix}$.
            \State $\bsym{\tilde{\Delta}} = \left( Q^T \odot (WS_u^T)+ P^T\odot (HS_v^T) \right)\bsym{1}_{d\times 1}, \bsym{\tilde{\Delta}}^\prime = \left( (WS_u^T) \odot (HS_v^T) \right)\bsym{1}_{d\times 1}$
            \State Parpare variables listed in \eqref{eq:LsRequire1}.
            %\State Parpare variables listed in \eqref{eq:LsRequireU} and $\bsym{\tilde{\Delta}} = \left( \pointprod{Q^T}{(WS_u^T)}+\pointprod{P^T}{(HS_v^T)} \right)\bsym{1}_{d\times 1}$
            \For { $\theta\gets\{1,\beta,\beta^2,\dots\}$ }
                %\State $\delta \gets \frac{\lambda'}{2}\left( 2\theta \dotprod{U}{S} + \theta^2\|S\|_F^2 \right) +\sum_{i=1}^l \log\left(1+e^{-y_i(\tilde{y}_i+\theta\tilde{\Delta}_i)}\right)$
                %\vspace{-.1cm}
                %\Statex \hspace{1.125cm}\phantom{ $\delta \gets$}$- \sum_{i=1}^l \sum_{i=1}^l \log\left(1+e^{-y_i\tilde{y}_i}\right)$.
                \State $\delta \gets \frac{\lambda}{2}\left( 2\theta \langle U, S_u \rangle + 2\theta \langle V, S_v \rangle + \theta^2\|S\|_F^2 \right) + \frac{1}{2} \sum_{i=1}^l \xi(\tilde{y}_i+\theta\tilde{\Delta}_i+{\theta}^2\tilde{\Delta}_i^\prime;y_i) - \frac{1}{2}\sum_{i=1}^l \xi(\tilde{y}_i;y_i)$
                \If { $ \delta \le \theta \nu \langle G,S \rangle $}
                    \State $U \gets U +\theta S_u$, $V \gets V +\theta S_v$
                    \State $f \gets f+ \delta$
                    \State $\bsym{\tilde{y}} \gets \bsym{\tilde{y}}+\theta\bsym{\tilde{\Delta}} +{\theta}^2 \bsym{\tilde{\Delta}}^\prime$
                    \State $\bb \leftarrow \byy - \by $
                    \State {\bf break}
                \EndIf
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}

In practice, any optimization method needs a stopping condition.
Here, we consider a relative setting
\begin{equation}
    \|\nabla f(U,V)\|_F \le \epsilon \|\nabla f(U_{\text{init}},V_{\text{init}})\|_F,
    \label{eq:AntStop}
\end{equation}
where $0<\epsilon<1$ is a small stopping tolerance and $(U_{\text{init}},V_{\text{init}})$ indicates the initial point of the model. 
If $\|\nabla f(U,V)\|_F >0$, Algorithm \ref{alg:LrFramework} conducts at least one Newton step and the function value of $f(U,V)$ must be decreased. In other words, if the condition in \eqref{eq:AntStop} is not satisfied yet, our minimization procedure must continue to update $U$ and $V$ rather than stay at the same point.

We consider a preconditioner $M$ to approximately factorize $H$ such that $H \approx {M}{M}^T$, and then use CG to solve
\begin{equation}
    \hat{H} \hat{\bs}  = \hat{\bg},
    \label{eq:PcondSys}
\end{equation}
where $\hat{H}={M}^{-1}{H}{M}^{-T}$ and $\hat{\bg}={M}^{-1}{\bg}$.
Once $\hat{\bs}$ is found, the solution of (\ref{eq:HLE}) can be recovered by ${\bs}={M}^{-T}\hat{\bs}$. 
Then, we consider the following diagonal preconditioner for solving \eqref{eq:HLE} as an example.
\begin{equation}
    {M}={M}^T=\diag({\bsym{h}}),
    \label{eq:Conder}
\end{equation}
where
\begin{equation*}
    {\bsym{h}} = \sqrt{ {\lambda} \bsym{1}_{d(M+N) \times 1} + \sum_{i=1}^l {\bj_i} \odot {\bj_i} }.
\end{equation*}
Note that ``$\sqrt{\cdot}$'' element-wisely performs the square-root operation if the input argument is a vector or a matrix. From 
\begin {align}
\bj_i &= \begin{bmatrix} \bw_i\otimes \bq_i \\ \bh_i\otimes \bp_i \end{bmatrix} = \begin{bmatrix} \vectorize(\bq_i \bw_i^T) \\ \vectorize(\bp_i \bh_i^T) \end{bmatrix}
\label{eq:Jacob_},
\end{align}
we have
\begin{align}
\sum_{i=1}^l {\bj_i} \odot {\bj_i}
&= \sum_{i=1}^l \begin{bmatrix} \vectorize \bigl((\bq_i \odot \bq_i) (\bw_i \odot \bw_i)^T \bigr) \\ \vectorize \bigl((\bp_i \odot \bp_i) (\bh_i \odot \bh_i)^T \bigr) \end{bmatrix} \nonumber  \\
&= \begin{bmatrix} \vectorize \bigl((Q \odot Q) (W \odot W) \bigr) \\ \vectorize \bigl((P \odot P) (H \odot H) \bigr) \end{bmatrix}
\label{eq:jj}.
\end{align}
Thus, the preconditioner can be obtained via
\begin{equation}
    {M}={M}^T=\diag \Bigl( \sqrt{ {\lambda} \bsym{1}_{d(M+N) \times 1} + \begin{bmatrix} \vectorize \bigl((Q \odot Q) (W \odot W) \bigr) \\ \vectorize \bigl((P \odot P) (H \odot H) \bigr) \end{bmatrix} } \Bigr).
    \label{eq:Conder}
\end{equation}

\begin{algorithm}[t]
    \caption{A preconditioned conjugate gradient method for solving \eqref{eq:HLE} by operations on matrix variables.}
    \label{alg:Pcg}
    \begin{algorithmic}[1]
        \State Given $0<\eta<1$ and $G$, the gradient matrix form of \eqref{eq:reMF}. Let $\hat{S}=\bsym{0}_{d\times (M+N)}$.
%        \State Compute $M$ via \eqref{eq:Conder}.
        %\State Calculate $\hat{G} = G \prescript{}{\cdot}{/} M$, $R=-\hat{G}$, $\gamma=\no{R}_F^2$, and $\hat{D}=R$.
        \State Calculate $R = -G$, $\hat{D}=R$, and $\gamma^0=\gamma=\|R\|_F^2$.
        \While {$\sqrt{\gamma} > \eta \sqrt{\gamma^0}$}
            \State $\hat{D}_h \gets \hat{D} $
            \State $\hat{\bz}\gets \left( {Q^T}\odot{(W\hat{D}_{hu}^T)}+{P^T}\odot{(H\hat{D}_{hv}^T)} \right) \bsym{1}_{d\times 1}$
            \State $\hat{D}_h \gets \left(\lambda \hat{D}_h + \begin{bmatrix} Q \bigl(\diag(\hat{\bz}) W \bigr) \, P \bigl ( \diag(\hat{\bz}) H \bigr) \end{bmatrix}\right) $
            \State $\alpha \gets \gamma / \langle \hat{D},\hat{D}_h \rangle$
            \State $\hat{S} \gets\hat{S}+\alpha \hat{D}$
            \State $R \gets R-\alpha \hat{D}_h$
            \State $\gamma^{\text{new}} \gets \|R\|_F^2$
            \State $\beta \gets \gamma^{\text{new}}/\gamma$
            \State $\hat{D} \gets R+\beta \hat{D}$
            \State $\gamma \gets \gamma^{\text{new}}$
        \EndWhile
        \State Output $\hat{S}$ as the solution.
    \end{algorithmic}
\end{algorithm}

\section{Experiments}
We aim to improve the test performance in the case which \cite{test17a} face, so first it need to reproduce the imbalance data like CTR data. Then, we conduct experiments to compare Gauss-Newton method with ALS and ANT.  
\subsection{Data Sets and Parameters}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|l|c|c|c|l|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\tt movielens10m}                                    & \multicolumn{4}{c|}{\tt neflix}                                             & A                       & B                       & C                                            \\ \hline
\multirow{2}{*}{rating} & \multicolumn{3}{c|}{number of instances}    & \multirow{2}{*}{rating} & \multicolumn{3}{c|}{number of instances}      & \multirow{2}{*}{rating} & \multirow{2}{*}{rating} & \multicolumn{1}{l|}{\multirow{2}{*}{rating}} \\ \cline{2-4} \cline{6-8}
                        & tr      & \multicolumn{1}{c|}{val} & te     &                         & tr       & \multicolumn{1}{c|}{val} & te      &                         &                         & \multicolumn{1}{l|}{}                        \\ \hline
0.5$\sim$2              & 1109741 & 139079                   & 138932 & 1                       & 3695710  & 460960                   & 461320  & 1                       & 1                       & 1                                            \\ \hline
2.5$\sim$5              & 6890302 & 860926                   & 861074 & 2$\sim$5                & 76688695 & 9587090                  & 9586732 & 5                       & 50                      & 100                                          \\ \hline
\end{tabular}
\caption{Distributions of ratings.}
\label{tab:data_dist}
\end{table}
To reproduce bad cases in ANT and ANT-P, we rebuild imbalance datas from {\tt movielens10m} and {\tt netflix}. For each data set we merge the original training set and test set, and then randomly split it into new training set, validation set, and test set by ratio $8:1:1$. To stress the effect of imbalance in data, we change those ratings which are less than or equal to $2$ to $1$ and change the other ratings to $5$ from original {\tt movielens10m} data. Similarly, we change those ratings which are large than $1$ to $5$ from original {\tt netflix}. In addition, we further create $4$ kinds of datasets from the rebuilt datasets by changing those ratings which are $5$ into $50$ and $100$. The distribution of ratings of these imbalance data shows in Table \ref{tab:data_dist}.
\begin{table}[H]
    \centering    
    \begin{tabular}{c|ccccc|ccccc|ccccc}
rating        & \multicolumn{5}{c|}{A}            & \multicolumn{5}{c|}{B}    & \multicolumn{5}{c}{C}     \\ \hline
solver        & acg  & als  & apcg & g     & pg   & acg & als & apcg & g & pg & acg & als & apcg & g & pg \\ \hline
learning rate & 0.09 & 0.09 & 0.09 & 0.065 & 0.08 & *   & *   & *    & * & *  & *   & *   & *    & * & *  \\
iteration     & 31   & 15   & 37   & 18    & 12   & *   & *   & *    & * & *  & *   & *   & *    & * & * 
\end{tabular}
    \caption{Parameters on different ratings of {\tt movielens10m}.}
    \label{tab:parameters_ml}
\end{table}
\begin{table}[H]
    \centering    
    \begin{tabular}{c|ccccc|ccccc|ccccc}
rating        & \multicolumn{5}{c|}{A}    & \multicolumn{5}{c|}{B}    & \multicolumn{5}{c}{C}     \\ \hline
solver        & acg & als & apcg & g & pg & acg & als & apcg & g & pg & acg & als & apcg & g & pg \\ \hline
learning rate & *   & *   & *    & * & *  & *   & *   & *    & * & *  & *   & *   & *    & * & *  \\
iteration     & *   & *   & *    & * & *  & *   & *   & *    & * & *  & *   & *   & *    & * & * 
\end{tabular}
    \caption{Parameters on different ratings of {\tt netflix}.}
    \label{tab:parameters_nf}
\end{table}
We find learning rate and outer iteration in each solver by using grid search on validation set and training set, and set $k=100$ in following experiments. The parameters shows in Table \ref{tab:parameters_ml} and Table \ref{tab:parameters_nf}.

\subsection{Frequency-aware Regularization}
It's well-known that give frequent user/item more penalties can lead to better testing performance on MF problem. In our experiments, we replace the regularization term in \eqref{eq:reMF} and get the following new formulation
\begin{equation}
    %\min_{U, V} \quad f(U, V)
    \min_{U,V}  \quad \frac{1}{2}\sum_{i}^l  \Bigl(y_i - (U \bw_i)^T (V \bh_i)\Bigr)^2+
    \frac{\lambda}{2} (\sum^{M}_m |\Omega_{m}| \|\bu_m\|^2 +\sum^{N}_{n} |\Omega_{n}| \|\bv_n\|^2).
    \label{eq:faMF}
\end{equation}
\subsection{Evaluation and Comparison on Test Performance}

\clearpage\newpage
\bibliographystyle{abbrvnat}
\bibliography{sdp,test}
%\bibliography{test}

\end{document}
